---
author: Andrew Liszewski
canonical_url: https://www.theverge.com/news/667613/ray-ban-meta-smart-glasses-ai-detailed-responses-call-a-volunteer
date: '2025-05-15T13:47:59'
excerpt: Meta announced two new features designed to assist blind or low vision users
  by leveraging the Ray-Ban Meta smart glassesâ€™ camera and its access to Meta AI.
  The news came as part of Global Accessibility Awareness Day. Rolling out to all
  users in the US and Canada in the coming weeks, Meta AI can now [&#8230;]
image: assets/media/WAEDMM9imKHi1pOxjr1G-g-5QrFVcfCKsTj_xS4CIaPBA.webp
source: theverge
tags:
- Gadgets
- Meta
- News
- Tech
- Wearable
title: Meta&#8217;s smart glasses can now describe what you&#8217;re seeing in more
  detail
---
<div><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">Meta announced <a href="https://about.fb.com/news/2025/05/advancing-accessibility-meta/">two new features designed to assist blind or low vision users</a> by leveraging the Ray-Ban Meta smart glasses&#8217; camera and its access to Meta AI. The news came as part of <a href="https://accessibility.day/">Global Accessibility Awareness Day</a>.</p><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">Rolling out to all users in the US and Canada in the coming weeks, Meta AI can now be customized to provide more detailed descriptions of what&#8217;s in front of users when they ask the smart assistant about their environment. In a <a href="https://about.fb.com/news/2025/05/advancing-accessibility-meta/">short video shared alongside the announcement</a>, Meta AI goes into more detail about the features of a waterside park, including describing grassy areas as being &#8220;well manicured.&#8221;</p><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">The feature can be activated by turning on &#8220;detailed responses&#8221; in the Accessibility section of the Device settings in the Meta AI app. Although it&#8217;s currently limited to users in the US and Canada, Meta says detailed responses will &#8220;expand to additional markets in the future,&#8221; but provided no details about when or which countries would get it next.</p><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">First <a href="https://www.bemyeyes.com/news/be-my-eyes-and-meta-announce-accessibility-partnership/">announced last September</a> as part of a partnership with the Be My Eyes organization and <a href="https://www.bemyeyes.com/news/be-my-eyes-is-rolling-out-on-ray-ban-meta-glasses-starting-today/">released last November in a limited rollout</a> that included the US, Canada, UK, Ireland, and Australia, Meta also confirmed today that its Call a Volunteer feature will &#8220;launch in all 18 countries where Meta AI is supported later this month.&#8221;</p><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">Blind and low vision users of the Ray-Ban Meta smart glasses can use the feature to connect to a network of over 8 million sighted volunteers and get assistance with everyday tasks such as following a recipe or locating an item on a shelf. By saying, &#8220;Hey Meta, Be My Eyes,&#8221; a volunteer will be able to see a user&#8217;s surroundings through a live feed from the glasses&#8217; camera and can provide descriptions or other assistance through its open-ear speakers.</p></div>