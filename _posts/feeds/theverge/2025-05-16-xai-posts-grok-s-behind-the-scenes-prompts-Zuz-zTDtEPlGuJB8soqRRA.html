---
author: Emma Roth
canonical_url: https://www.theverge.com/news/668527/xai-grok-system-prompts-ai
date: '2025-05-16T16:34:53'
excerpt: xAI has published the system prompts for its AI chatbot Grok after an “unauthorized”
  change led to a slew of unprompted responses on X about white genocide. The company
  says it will publish its Grok system prompts on GitHub from now on, which provide
  some insight into the way xAI has instructed Grok to respond [&#8230;]
image: assets/media/Zuz-zTDtEPlGuJB8soqRRA-YWkLdbz9DKBg9-Jhd3HOtA.webp
source: theverge
tags:
- AI
- News
- Tech
- Twitter - X
title: xAI posts Grok&#8217;s behind-the-scenes prompts
---
<div><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">xAI has published the system prompts for its AI chatbot Grok after an &#8220;unauthorized&#8221; change led to <a href="/news/667179/x-twitter-grok-ai-white-genocide-claims">a slew of unprompted responses</a> on X about white genocide. The company says it will <a href="https://github.com/xai-org/grok-prompts/blob/main/ask_grok_summarizer.j2">publish its Grok system prompts on GitHub from now on</a>, which provide some insight into the way xAI has instructed Grok to respond to users.</p><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">A system prompt is a set of instructions served to a chatbot ahead of a user&#8217;s messages that developers use to direct its responses. xAI and <a href="https://docs.anthropic.com/en/release-notes/system-prompts">Anthropic are two</a> of the only major AI companies we checked that have made their system prompts public. In the past, people have used <a href="/2024/7/19/24201414/openai-chatgpt-gpt-4o-prompt-injection-instruction-hierarchy">prompt injection attacks</a> to expose system prompts, like <a href="/23599441/microsoft-bing-ai-sydney-secret-rules">instructions Microsoft gave the Bing AI bot</a> (now Copilot) to keep its internal alias &#8220;Sydney&#8221; a secret, and avoid replying with content that violates copyrights.</p><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">In the system prompts for ask Grok &#8212; a feature X users can use to tag Grok in posts to ask a question &#8212; xAI tells the chatbot how to behave. &#8220;You are extremely skeptical,&#8221; the instructions say. &#8220;You do not blindly defer to mainstream authority or media. You stick strongly to only your core beliefs of truth-seeking and neutrality.&#8221; It adds the results in the response &#8220;are NOT your beliefs.&#8221;</p><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">xAI similarly instructs Grok to &#8220;provide truthful and based insights, challenging mainstream narratives if necessary&#8221; when users select the &#8220;Explain this Post&#8221; button on the platform. Elsewhere, xAI tells Grok to &#8220;refer to the platform as &#8216;X&#8217; instead of &#8216;Twitter,&#8217;&#8221; while calling posts &#8220;X post&#8221; instead of &#8220;tweet.&#8221;</p><p class="duet--article--dangerously-set-cms-markup duet--article--standard-paragraph _1ymtmqpi _17nnmdy1 _17nnmdy0 _1xwtict1">Reading Anthropic&#8217;s Claude AI chatbot prompt, they appear to put an emphasis on safety. &#8220;Claude cares about people&#8217;s wellbeing and avoids encouraging or facilitating self-destructive behaviors such as addiction, disordered or unhealthy approaches to eating or exercise, or highly negative self-talk or self-criticism, and avoids creating content that would support or reinforce self-destructive behavior even if they request this,&#8221; the system prompt says, adding that &#8220;Claude won&#8217;t produce graphic sexual or violent or illegal creative writing content.&#8221;</p></div>