---
author: Kyle Wiggers
canonical_url: https://techcrunch.com/2025/05/12/improvements-in-reasoning-ai-models-may-slow-down-soon-analysis-finds/
date: '2025-05-12T22:36:11'
excerpt: An analysis by Epoch AI, a nonprofit AI research institute, suggests the
  AI industry may not be able to eke massive performance gains out of reasoning AI
  models for much longer. As soon as within a year, progress from reasoning models
  could slow down, according to the report&#8217;s findings. Reasoning models such
  as OpenAI&#8217;s o3 [&#8230;]
image: assets/media/uvwiUfIA56WPSs2WEyNHxw-4r3HFELP4rBG6Zw80fy1jw.webp
source: techcrunch
tags:
- AI
- epoch ai
- reasoning models
title: Improvements in ‘reasoning’ AI models may slow down soon, analysis finds
---
<div>
<p id="speakable-summary" class="wp-block-paragraph">An <a href="https://epoch.ai/gradient-updates/how-far-can-reasoning-models-scale" target="_blank" rel="noreferrer noopener nofollow">analysis</a> by Epoch AI, a nonprofit AI research institute, suggests the AI industry may not be able to eke massive performance gains out of reasoning AI models for much longer. As soon as within a year, progress from reasoning models could slow down, according to the report&#8217;s findings.</p>

<p class="wp-block-paragraph">Reasoning models such as OpenAI&#8217;s <a href="https://techcrunch.com/2025/04/16/openai-launches-a-pair-of-ai-reasoning-models-o3-and-o4-mini/">o3</a> have led to substantial gains on AI benchmarks in recent months, particularly benchmarks measuring math and programming skills. The models can apply more computing to problems, which can improve their performance, with the downside being that they take longer than conventional models to complete tasks.</p>

 


 


<p class="wp-block-paragraph">Reasoning models are developed by first training a conventional model on a massive amount of data, then applying a technique called reinforcement learning, which effectively gives the model &#8220;feedback&#8221; on its solutions to difficult problems. </p>

<p class="wp-block-paragraph">So far, frontier AI labs like OpenAI haven&#8217;t applied an enormous amount of computing power to the reinforcement learning stage of reasoning model training, according to Epoch. </p>

<p class="wp-block-paragraph">That&#8217;s changing. OpenAI has said that it applied around 10x more computing to train o3 than its predecessor, o1, and Epoch speculates that most of this computing was devoted to reinforcement learning. And OpenAI researcher Dan Roberts recently revealed that the company&#8217;s future plans call for <a href="https://www.youtube.com/watch?v=_rjD_2zn2JU&amp;list=PLOhHNjZItNnMEqGLRWkKjaMcdSJptkR08&amp;index=7" target="_blank" rel="noreferrer noopener nofollow">prioritizing reinforcement learning</a> to use far more computing power, even more than for the initial model training.</p>

<p class="wp-block-paragraph">But there&#8217;s still an upper bound to how much computing can be applied to reinforcement learning, per Epoch.</p>

 

<p class="wp-block-paragraph">Josh You, an analyst at Epoch and the author of the analysis, explains that performance gains from standard AI model training are currently quadrupling every year, while performance gains from reinforcement learning are growing tenfold every 3-5 months. The progress of reasoning training will &#8220;probably converge with the overall frontier by 2026,&#8221; he continues.</p>
 

<p class="wp-block-paragraph">Epoch&#8217;s analysis makes a number of assumptions, and draws in part on public comments from AI company executives. But it also makes the case that scaling reasoning models may prove to be challenging for reasons besides computing, including high overhead costs for research. </p>

<p class="wp-block-paragraph">&#8220;If there&#8217;s a persistent overhead cost required for research, reasoning models might not scale as far as expected,&#8221; writes You. &#8220;Rapid compute scaling is&#160;potentially&#160;a very important ingredient in reasoning model progress, so it&#8217;s worth tracking this closely.&#8221;</p>

 


			

			</div>