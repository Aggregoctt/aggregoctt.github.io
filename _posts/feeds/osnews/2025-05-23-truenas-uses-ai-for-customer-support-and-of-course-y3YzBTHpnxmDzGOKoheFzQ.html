---
author: Thom Holwerda
canonical_url: https://www.osnews.com/story/142417/truenas-uses-ai-for-customer-support-and-of-course-it-goes-horribly-wrong/
date: '2025-05-23T19:27:59'
excerpt: 'Let&#8217;s check in on TrueNAS, who apparently employ &#8220;AI&#8221;
  to handle customer service tickets. Kyle Kingsbury had to have dealings with TrueNAS&#8217;
  customer support, and it was a complete trashfire of irrelevance and obviously wrong
  answers, spiraling all the way into utter lies. The &#8220;AI&#8221; couldn&#8217;t
  generate its way out of a paper bag, and for a paying customer who is entitled to
  support, that&#8217;s not a great experience. Kingsbury concludes: I get it. Support
  is often viewed as a cost center, and agents are often working against a brutal,
  endlessly increasing backlog of tickets. There is pressure at every level to clear
  those tickets in as little time as possible. Large Language Models create plausible
  support responses with incredible speed, but their output must still be reviewed
  by humans. Reviewing large volumes of plausible, syntactically valid text for factual
  errors is exhausting, time-consuming work, and every few minutes a new ticket arrives.
  Companies must do more with less; what was once a team of five support engineers
  becomes three. Pressure builds, and the time allocated to review the LLM’s output
  becomes shorter and shorter. Five minutes per ticket becomes three. The LLM gets
  it mostly right. Two minutes. Looks good. Sixty seconds. Click submit. There are
  one hundred eighty tickets still in queue, and behind every one is a disappointed
  customer, and behind that is the risk of losing one’s job. Thirty seconds. Submit.
  Submit. The metrics do not measure how many times the system has lied to customers.
  ↫ Kyle Kingsbury This time, it&#8217;s just about an upgrade process for a NAS,
  and the worst possible outcome &#8220;AI&#8221; generated bullshit could lead to
  is a few lost files. Potentially disastrous on a personal level for the customer
  involved, but not exactly a massive problem. However, once we&#8217;re talking support
  for medical devices, medication, dangerous power tools, and worse, this could &#8211;
  and trust me, will &#8211; lead to injury and death. TrueNAS, for its part, contacted
  Kingsbury after his blog post blew up, and assured him that &#8220;their support
  process does not normally incorporate LLMs&#8221;, and that they would investigate
  internally what, exactly, happened. I hope the popularity of Kingsbury&#8217;s post
  has jolted whomever is responsible for customer service at TrueNAS that farming
  out customer service to text generators is a surefire way to damage your reputation.'
image: null
source: osnews
tags:
- Clown car
title: TrueNAS uses “AI” for customer support, and of course it goes horribly wrong
---
<div><p>Let&#8217;s check in on TrueNAS, who apparently employ &#8220;AI&#8221; to handle customer service tickets. Kyle Kingsbury had to have dealings with TrueNAS&#8217; customer support, and it was a complete trashfire of irrelevance and obviously wrong answers, spiraling all the way into utter lies. The &#8220;AI&#8221; couldn&#8217;t generate its way out of a paper bag, and for a paying customer who is entitled to support, that&#8217;s not a great experience.</p><p>Kingsbury concludes:</p><blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"><p>I get it. Support is often viewed as a cost center, and agents are often working against a brutal, endlessly increasing backlog of tickets. There is pressure at every level to clear those tickets in as little time as possible. Large Language Models create plausible support responses with incredible speed, but their output must still be reviewed by humans. Reviewing large volumes of plausible, syntactically valid text for factual errors is exhausting, time-consuming work, and every few minutes a new ticket arrives.</p><p>Companies must do more with less; what was once a team of five support engineers becomes three. Pressure builds, and the time allocated to review the LLM&#8217;s output becomes shorter and shorter. Five minutes per ticket becomes three. The LLM gets it mostly right. Two minutes. Looks good. Sixty seconds. Click submit. There are one hundred eighty tickets still in queue, and behind every one is a disappointed customer, and behind that is the risk of losing one&#8217;s job. Thirty seconds. Submit. Submit. The metrics do not measure how many times the system has lied to customers.</p>
<a href="https://aphyr.com/posts/387-the-future-of-customer-support-is-lies-i-guess">&#8619; Kyle Kingsbury</a></blockquote><p>This time, it&#8217;s just about an upgrade process for a NAS, and the worst possible outcome &#8220;AI&#8221; generated bullshit could lead to is a few lost files. Potentially disastrous on a personal level for the customer involved, but not exactly a massive problem. However, once we&#8217;re talking support for medical devices, medication, dangerous power tools, and worse, this could &#8211; and trust me, will &#8211; lead to injury and death.</p><p>TrueNAS, for its part, contacted Kingsbury after his blog post blew up, and assured him that &#8220;their support process does not normally incorporate LLMs&#8221;, and that they would investigate internally what, exactly, happened. I hope the popularity of Kingsbury&#8217;s post has jolted whomever is responsible for customer service at TrueNAS that farming out customer service to text generators is a surefire way to damage your reputation.</p></div>